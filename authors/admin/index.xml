<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yong Jiang</title>
    <link>https://jiangyong.site/authors/admin/</link>
      <atom:link href="https://jiangyong.site/authors/admin/index.xml" rel="self" type="application/rss+xml" />
    <description>Yong Jiang</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language>
    <image>
      <url>https://jiangyong.site/img/icon-192.png</url>
      <title>Yong Jiang</title>
      <link>https://jiangyong.site/authors/admin/</link>
    </image>
    
    <item>
      <title></title>
      <link>https://jiangyong.site/authors/admin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jiangyong.site/authors/admin/</guid>
      <description>&lt;p&gt;Hi! I currently work at &lt;a href=&#34;https://damo.alibaba.com/&#34; target=&#34;_blank&#34;&gt;Alibaba DAMO Academy&lt;/a&gt;. I received my Ph.D from the joint program of ShanghaiTech University and University of Chinese Academy of Sciences. I was very fortunate to be advised by &lt;a href=&#34;http://faculty.sist.shanghaitech.edu.cn/faculty/tukw/&#34; target=&#34;_blank&#34;&gt;Prof. Kewei Tu&lt;/a&gt;. I am interested in machine learning and natural language processing.&lt;/p&gt;

&lt;p&gt;My current research mainly focuses on entity understanding tasks, information retrieval (query/doc understanding), language model pretraining, multilingual NLP, structured prediction and so on. Furthermore, I also ship these cutting-edge technologies to real products and platforms.&lt;/p&gt;

&lt;p&gt;In my PhD time, I mainly worked on learning latent variable models for NLP problems and ML problems.&lt;/p&gt;

&lt;p&gt;Spotlight of our recent work:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Incorporating various kinds of knowledge to improve named entity recognition: &lt;a href=&#34;https://arxiv.org/abs/2009.08330&#34; target=&#34;_blank&#34;&gt;embedding combination&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2010.05006&#34; target=&#34;_blank&#34;&gt;ACE&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2105.03654&#34; target=&#34;_blank&#34;&gt;retrieval guided learning&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2203.00545&#34; target=&#34;_blank&#34;&gt;sparse retrieval&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2112.06482&#34; target=&#34;_blank&#34;&gt;multi-modal NER&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Knowledge distillation for learning multilingual models: &lt;a href=&#34;https://arxiv.org/abs/2004.03846&#34; target=&#34;_blank&#34;&gt;structure-level KD&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2010.05010&#34; target=&#34;_blank&#34;&gt;structural KD&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Improving sequence labeling methods: &lt;a href=&#34;https://arxiv.org/abs/2011.05604&#34; target=&#34;_blank&#34;&gt;designing powerful potential functions&lt;/a&gt;, &lt;a href=&#34;https://openreview.net/pdf?id=ZrBZrf-BS3Z&#34; target=&#34;_blank&#34;&gt;speeding up CRF training &amp;amp; inference&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Leveraging source models to improve cross-lingual ability: &lt;a href=&#34;https://aclanthology.org/2021.acl-long.380/&#34; target=&#34;_blank&#34;&gt;risk minimization&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.org/2021.acl-long.207/&#34; target=&#34;_blank&#34;&gt;multi-view learning&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.338/&#34; target=&#34;_blank&#34;&gt;word reordering&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Unsupervised grammar induction: &lt;a href=&#34;https://aclanthology.org/D16-1073/&#34; target=&#34;_blank&#34;&gt;the first neural-based unsupervised parser&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1708.01018&#34; target=&#34;_blank&#34;&gt;discriminative autoencoder&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2010.14720&#34; target=&#34;_blank&#34;&gt;2nd order parsing&lt;/a&gt;, &lt;a href=&#34;https://github.com/tukw/unsupervised-parsing-tutorial&#34; target=&#34;_blank&#34;&gt;EACL tutorial&lt;/a&gt; and &lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.300/&#34; target=&#34;_blank&#34;&gt;empirical study&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Multi-view learning for &lt;a href=&#34;https://arxiv.org/abs/2105.03654&#34; target=&#34;_blank&#34;&gt;NER&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2109.05716&#34; target=&#34;_blank&#34;&gt;entity linking&lt;/a&gt; and &lt;a href=&#34;https://aclanthology.org/2021.acl-long.207/&#34; target=&#34;_blank&#34;&gt;cross-lingual learning&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Fun with KL divergence: &lt;a href=&#34;https://aclanthology.org/2021.acl-long.207/&#34; target=&#34;_blank&#34;&gt;KL(p(*|a, b, c) || p(*|d, e))&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2004.03846&#34; target=&#34;_blank&#34;&gt;KL(P || p)&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2010.05010&#34; target=&#34;_blank&#34;&gt;KL(p || q)&lt;/a&gt;, &lt;a href=&#34;https://openreview.net/pdf?id=ZrBZrf-BS3Z&#34; target=&#34;_blank&#34;&gt;KL(tractable || intractable?)&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2112.06482&#34; target=&#34;_blank&#34;&gt;KL (different modality)&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We have some research intern positions available in &lt;a href=&#34;https://damo.alibaba.com/&#34; target=&#34;_blank&#34;&gt;Alibaba DAMO Academy&lt;/a&gt;. If you are interested in NLP and ML, please feel free to contact me: jiangyong.ml@gmail.com.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
